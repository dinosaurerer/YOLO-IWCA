import cv2
import pyttsx3
import numpy as np
from PIL import Image
import torch
from ultralytics import YOLO

engine = pyttsx3.init()
engine.setProperty('rate', 160)
engine.setProperty('volume', 1.0)

import threading
# å¼‚æ­¥è¯­éŸ³æ’­æŠ¥å‡½æ•°ï¼Œé¿å…é˜»å¡å’Œ run loop å†²çª
tts_lock = threading.Lock()


def add_rounded_corners(image, corner_radius=20, border_color=(0, 0, 0)):
    """
    ä¸ºå›¾åƒæ·»åŠ é€æ˜åœ†è§’è¾¹æ¡†
    :param image: åŸå§‹BGRå›¾åƒ
    :param corner_radius: åœ†è§’åŠå¾„
    :param border_color: è¾¹æ¡†é¢œè‰²
    :return: å¸¦åœ†è§’çš„BGRAæ ¼å¼å›¾åƒ
    """
    # è½¬æ¢ä¸ºBGRAæ ¼å¼
    h, w = image.shape[:2]
    bgra = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)

    # åˆ›å»ºé€æ˜è¾¹æ¡†
    border_size = corner_radius
    new_h = h + border_size * 2
    new_w = w + border_size * 2
    bordered = cv2.copyMakeBorder(bgra, border_size, border_size, border_size, border_size,
                                  cv2.BORDER_CONSTANT, value=(0, 0, 0, 0))

    # åˆ›å»ºåœ†è§’è’™ç‰ˆ
    mask = np.zeros((new_h, new_w), dtype=np.uint8)
    cv2.rectangle(mask, (corner_radius, 0), (new_w - corner_radius, new_h), 255, -1)
    cv2.rectangle(mask, (0, corner_radius), (new_w, new_h - corner_radius), 255, -1)
    cv2.circle(mask, (corner_radius, corner_radius), corner_radius, 255, -1)
    cv2.circle(mask, (new_w - corner_radius, corner_radius), corner_radius, 255, -1)
    cv2.circle(mask, (corner_radius, new_h - corner_radius), corner_radius, 255, -1)
    cv2.circle(mask, (new_w - corner_radius, new_h - corner_radius), corner_radius, 255, -1)

    # åº”ç”¨è’™ç‰ˆ
    bordered[:, :, 3] = cv2.bitwise_and(bordered[:, :, 3], mask)

    # æ·»åŠ å¤–è¾¹æ¡†
    cv2.rectangle(bordered, (0, 0), (new_w - 1, new_h - 1), (*border_color, 255), 1)
    return bordered



def speak_trash_tip(text):
    def _speak():
        with tts_lock:
            local_engine = pyttsx3.init()  # ç”¨å±€éƒ¨å¼•æ“é˜²æ­¢å…¨å±€çŠ¶æ€æ±¡æŸ“
            local_engine.setProperty('rate', 160)
            local_engine.setProperty('volume', 1.0)
            try:
                local_engine.say(text)
                local_engine.runAndWait()
            except RuntimeError:
                pass
            finally:
                del local_engine

    threading.Thread(target=_speak, daemon=True).start()

class YOLOv8Detector:
    def __init__(self, model_path=None, img_size=640, conf_thres=0.25, iou_thres=0.45):
        self.model = None
        self.img_size = img_size
        self.conf_thres = conf_thres
        self.iou_thres = iou_thres
        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

        if model_path:
            self.load_model(model_path)

    def load_model(self, model_path):
        try:
            self.model = YOLO(model_path)
            self.model.to(self.device)
            self.names = self.model.names
            return True, "æ¨¡å‹åŠ è½½æˆåŠŸ!"
        except Exception as e:
            return False, f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

    def detect_image(self, image):
        if self.model is None:
            return None, "è¯·å…ˆåŠ è½½æ¨¡å‹!"

        try:
            if isinstance(image, Image.Image):
                img = np.array(image)
            else:
                img = image.copy()

            results = self.model.predict(
                source=img,
                imgsz=self.img_size,
                conf=self.conf_thres,
                iou=self.iou_thres,
                device=self.device
            )

            result_img = results[0].plot()
            result_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)

            detection_info = []
            for det in results[0].boxes:
                x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())
                conf = float(det.conf[0])
                cls = int(det.cls[0])
                detection_info.append({
                    'class': self.names[cls],
                    'confidence': conf,
                    'bbox': [x1, y1, x2, y2]
                })

            return result_img_rgb, detection_info
        except Exception as e:
            return None, f"å›¾ç‰‡æ£€æµ‹å¤±è´¥: {str(e)}"

    def process_video(self, video_path):
        if self.model is None:
            return None, "è¯·å…ˆåŠ è½½æ¨¡å‹!"

        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            output_path = video_path.replace(".mp4", "_output.mp4")
            # st.write(f"è¾“å‡ºè§†é¢‘è·¯å¾„: {output_path}")
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                results = self.model.predict(
                    source=frame,
                    imgsz=self.img_size,
                    conf=self.conf_thres,
                    iou=self.iou_thres,
                    device=self.device
                )

                result_img = results[0].plot()
                out.write(result_img)

            cap.release()
            out.release()
            return output_path, None
        except Exception as e:
            return None, f"è§†é¢‘å¤„ç†å¤±è´¥: {str(e)}"



# åˆå§‹åŒ–æ¨¡å‹
model_path = "../weights/yolo11-365-noaug.pt"  # æ›¿æ¢ä¸ºå®é™…æ¨¡å‹è·¯å¾„
detector = YOLOv8Detector(
            model_path=model_path,
            img_size=640,
            conf_thres=0.25,
            iou_thres=0.45
        )

# åƒåœ¾åˆ†ç±»æç¤ºä¿¡æ¯ (ç¤ºä¾‹)
TRASH_HINTS = {
    "recyclable waste": {"tip": "è¯·æŠ•æ”¾è‡³è“è‰²å¯å›æ”¶åƒåœ¾æ¡¶ã€‚", "color": "è“è‰² â™»ï¸"},
    "hazardous waste": {"tip": "è¯·æŠ•æ”¾è‡³çº¢è‰²æœ‰å®³åƒåœ¾æ¡¶ã€‚", "color": "çº¢è‰² â˜£ï¸"},
    "kitchen waste": {"tip": "è¯·æŠ•æ”¾è‡³ç»¿è‰²å¨ä½™åƒåœ¾æ¡¶ã€‚", "color": "ç»¿è‰² ğŸ¥¦"},
    "other waste": {"tip": "è¯·æŠ•æ”¾è‡³ç°è‰²å…¶ä»–åƒåœ¾æ¡¶ã€‚", "color": "ç°è‰² ğŸ—‘ï¸"}
}


# æœ¬åœ°æ‘„åƒå¤´æ£€æµ‹å‡½æ•°
def run_camera_detection():
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return

    last_spoken_classes = set()

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("æ‘„åƒå¤´è¯»å–å¤±è´¥")
                break

            # æ¨¡å‹æ¨ç†

            results = detector.model.predict(
                source=frame,
                imgsz=detector.img_size,
                conf=detector.conf_thres,
                iou=detector.iou_thres,
                device=detector.device
            )

            # æ˜¾ç¤ºç»“æœ
            result_img = results[0].plot()
            # æ·»åŠ åœ†è§’æ•ˆæœ
            decorated_img = add_rounded_corners(result_img,
                                                corner_radius=20,
                                                border_color=(100, 100, 100))

            # è½¬æ¢å›BGRæ ¼å¼ï¼ˆéƒ¨åˆ†ç¯å¢ƒå¯èƒ½ä¸æ”¯æŒé€æ˜é€šé“æ˜¾ç¤ºï¼‰
            final_display = cv2.cvtColor(decorated_img, cv2.COLOR_BGRA2BGR)

            cv2.imshow('Detection', final_display)

            # åŸå§‹è¯­éŸ³é€»è¾‘ï¼ˆå®Œå…¨ä¿æŒæ‚¨çš„å®ç°ï¼‰
            detected_classes = [detector.model.names[int(box.cls)] for box in results[0].boxes]
            if detected_classes:
                for cls in set(detected_classes):
                    if cls in TRASH_HINTS and cls not in last_spoken_classes:
                        speak_trash_tip(TRASH_HINTS[cls]["tip"])
                        last_spoken_classes.add(cls)
            else:
                last_spoken_classes.clear()

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    finally:
        cap.release()
        cv2.destroyAllWindows()


# è¿è¡Œæ‘„åƒå¤´æ£€æµ‹
if __name__ == "__main__":
    run_camera_detection()